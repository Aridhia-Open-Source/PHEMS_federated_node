Release "fn" does not exist. Installing it now.
NAME: fn
LAST DEPLOYED: Fri Dec 12 16:59:42 2025
NAMESPACE: fn
STATUS: pending-install
REVISION: 1
HOOKS:
---
# Source: federated-node/templates/keycloak-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: keycloak
  labels:
    app.kubernetes.io/managed-by: Helm
  annotations:
    helm.sh/hook: pre-install
    helm.sh/hook-weight: "-6"
    meta.helm.sh/release-name: fn
    meta.helm.sh/release-namespace: fn
---
# Source: federated-node/templates/keycloak-start-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: keycloak-start-config
  namespace: keycloak
  labels:
    app.kubernetes.io/managed-by: Helm
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-5"
    helm.sh/resource-policy: keep
    meta.helm.sh/release-name: fn
    meta.helm.sh/release-namespace: fn
data:
  config.json: |-
    {
      "realm": "FederatedNode",
      "enabled": true,
      "registrationEmailAsUsername": false,
      "offlineSessionIdleTimeout": "${KEYCLOAK_TOKEN_LIFE}",
      "ssoSessionIdleTimeout": "${KEYCLOAK_TOKEN_LIFE}",
      "ssoSessionMaxLifespan": "${KEYCLOAK_TOKEN_LIFE}",
      "loginWithEmailAllowed": true,
      "duplicateEmailsAllowed": false,
      "resetPasswordAllowed": false,
      "attributes": {
        "userProfileEnabled": true
      },
      "groups": [
        {
          "name": "Administrator",
          "path": "/Administrator"
        },
        {
          "name": "Super Administrator",
          "path": "/Super Administrator"
        },
        {
          "name": "Users",
          "path": "/Users"
        }
      ],
      "requiredCredentials": [
        "password"
      ],
      "roles": {
        "realm": [
          {
            "name": "Administrator",
            "description": "Administrators with limited, non-realm, related permissions",
            "composite": true,
            "composites": {
              "realm": [
                "Users"
              ],
              "client": {
                "realm-management": [
                  "view-users",
                  "view-clients",
                  "manage-users",
                  "view-authorization",
                  "query-groups",
                  "query-users",
                  "manage-clients",
                  "view-realm",
                  "manage-authorization",
                  "query-clients",
                  "view-events"
                ],
                "account": [
                  "view-groups",
                  "delete-account",
                  "manage-account",
                  "view-applications",
                  "view-profile",
                  "view-consent"
                ]
              }
            },
            "clientRole": false,
            "attributes": {}
          },
          {
            "name": "Super Administrator",
            "description": "Super administrators with extensive permissions set",
            "composite": true,
            "composites": {
              "realm": [
                "Administrator",
                "Users"
              ],
              "client": {
                "realm-management": [
                  "view-users",
                  "view-clients",
                  "manage-identity-providers",
                  "view-authorization",
                  "query-groups",
                  "query-users",
                  "view-realm",
                  "manage-authorization",
                  "query-clients",
                  "realm-admin",
                  "manage-users",
                  "manage-events",
                  "create-client",
                  "query-realms",
                  "view-identity-providers",
                  "impersonation",
                  "manage-realm",
                  "manage-clients",
                  "view-events"
                ],
                "broker": [
                  "read-token"
                ],
                "account": [
                  "manage-consent",
                  "delete-account",
                  "view-applications",
                  "view-consent",
                  "view-groups",
                  "manage-account",
                  "view-profile",
                  "manage-account-links"
                ]
              }
            },
            "clientRole": false,
            "attributes": {}
          },
          {
            "name": "Users",
            "description": "Basic Users",
            "composite": true,
            "composites": {
              "client": {
                "account": [
                  "view-profile",
                  "manage-account"
                ]
              }
            },
            "clientRole": false,
            "attributes": {}
          },
          {
            "name": "System",
            "description": "Users for integrations",
            "composite": true,
            "composites": {
              "client": {
                "account": [
                  "view-profile",
                  "manage-account"
                ]
              }
            },
            "clientRole": false,
            "attributes": {}
          }
        ]
      },
      "clients": [
        {
          "clientId": "global",
          "name": "Global",
          "description": "General access definition for endpoints",
          "rootUrl": "",
          "adminUrl": "",
          "baseUrl": "/",
          "enabled": true,
          "clientAuthenticatorType": "client-secret",
          "secret": "${KEYCLOAK_SECRET}",
          "redirectUris": [
            "/*"
          ],
          "webOrigins": [
            "http://keycloak.identities.svc.cluster.local",
            "${KEYCLOAK_HOSTNAME}"
          ],
          "attributes": {
            "client.session.idle.timeout": "${KEYCLOAK_TOKEN_LIFE}",
            "standard.token.exchange.enabled": true
          },
          "notBefore": 0,
          "bearerOnly": false,
          "consentRequired": false,
          "standardFlowEnabled": true,
          "implicitFlowEnabled": false,
          "directAccessGrantsEnabled": true,
          "serviceAccountsEnabled": true,
          "authorizationServicesEnabled": true,
          "publicClient": false,
          "frontchannelLogout": true,
          "protocol": "openid-connect",
          "defaultClientScopes": [
            "web-origins",
            "acr",
            "roles",
            "profile",
            "email"
          ],
          "optionalClientScopes": [
            "address",
            "phone",
            "offline_access",
            "microprofile-jwt"
          ],
          "authorizationSettings": {
            "allowRemoteResourceManagement": true,
            "policyEnforcementMode": "ENFORCING",
            "resources": [
              {
                "name": "endpoints",
                "ownerManagedAccess": false,
                "displayName": "",
                "attributes": {},
                "uris": [],
                "scopes": [
                  {
                    "name": "can_admin_dataset"
                  },
                  {
                    "name": "can_exec_task"
                  },
                  {
                    "name": "can_admin_task"
                  },
                  {
                    "name": "can_access_dataset"
                  },
                  {
                    "name": "can_transfer_token"
                  },
                  {
                    "name": "can_do_admin"
                  },
                  {
                    "name": "can_send_request"
                  },
                  {
                    "name": "can_admin_request"
                  }
                ],
                "icon_uri": ""
              }
            ],
            "policies": [
              {
                "name": "admin-policy",
                "description": "",
                "type": "role",
                "logic": "POSITIVE",
                "decisionStrategy": "UNANIMOUS",
                "config": {
                  "roles": "[{\"id\":\"Administrator\",\"required\":false},{\"id\":\"Super Administrator\",\"required\":false}]"
                }
              },
              {
                "name": "superadmin-policy",
                "description": "",
                "type": "role",
                "logic": "POSITIVE",
                "decisionStrategy": "UNANIMOUS",
                "config": {
                  "roles": "[{\"id\":\"Super Administrator\",\"required\":false}]"
                }
              },
              {
                "name": "all-users-policy",
                "description": "",
                "type": "role",
                "logic": "POSITIVE",
                "decisionStrategy": "UNANIMOUS",
                "config": {
                  "roles": "[{\"id\":\"Administrator\",\"required\":false},{\"id\":\"Users\",\"required\":false},{\"id\":\"Super Administrator\",\"required\":false}]"
                }
              },
              {
                "name": "system-policy",
                "description": "",
                "type": "role",
                "logic": "POSITIVE",
                "decisionStrategy": "UNANIMOUS",
                "config": {
                  "roles": "[{\"id\":\"System\",\"required\":false}]"
                }
              },
              {
                "name": "can admin dataset permission",
                "description": "",
                "type": "scope",
                "logic": "POSITIVE",
                "decisionStrategy": "AFFIRMATIVE",
                "config": {
                  "scopes": "[\"can_admin_dataset\"]",
                  "applyPolicies": "[\"admin-policy\", \"system-policy\"]"
                }
              },
              {
                "name": "can admin task permission",
                "description": "",
                "type": "scope",
                "logic": "POSITIVE",
                "decisionStrategy": "AFFIRMATIVE",
                "config": {
                  "scopes": "[\"can_admin_task\"]",
                  "applyPolicies": "[\"admin-policy\"]"
                }
              },
              {
                "name": "can admin request permission",
                "description": "",
                "type": "scope",
                "logic": "POSITIVE",
                "decisionStrategy": "AFFIRMATIVE",
                "config": {
                  "scopes": "[\"can_admin_request\"]",
                  "applyPolicies": "[\"admin-policy\", \"system-policy\"]"
                }
              },
              {
                "name": "can exec task permission",
                "description": "",
                "type": "scope",
                "logic": "POSITIVE",
                "decisionStrategy": "AFFIRMATIVE",
                "config": {
                  "scopes": "[\"can_exec_task\"]",
                  "applyPolicies": "[\"all-users-policy\", \"system-policy\"]"
                }
              },
              {
                "name": "can access dataset permission",
                "description": "",
                "type": "scope",
                "logic": "POSITIVE",
                "decisionStrategy": "AFFIRMATIVE",
                "config": {
                  "scopes": "[\"can_access_dataset\"]",
                  "applyPolicies": "[\"all-users-policy\", \"system-policy\"]"
                }
              },
              {
                "name": "can transfer token permission",
                "description": "",
                "type": "scope",
                "logic": "POSITIVE",
                "decisionStrategy": "AFFIRMATIVE",
                "config": {
                  "scopes": "[\"can_transfer_token\"]",
                  "applyPolicies": "[\"admin-policy\", \"system-policy\"]"
                }
              },
              {
                "name": "can do admin permission",
                "description": "",
                "type": "scope",
                "logic": "POSITIVE",
                "decisionStrategy": "AFFIRMATIVE",
                "config": {
                  "scopes": "[\"can_do_admin\"]",
                  "applyPolicies": "[\"admin-policy\"]"
                }
              },
              {
                "name": "can send request permission",
                "description": "",
                "type": "scope",
                "logic": "POSITIVE",
                "decisionStrategy": "AFFIRMATIVE",
                "config": {
                  "scopes": "[\"can_send_request\"]",
                  "applyPolicies": "[\"all-users-policy\", \"system-policy\"]"
                }
              }
            ],
            "scopes": [
              {
                "name": "can_admin_dataset"
              },
              {
                "name": "can_exec_task"
              },
              {
                "name": "can_admin_task"
              },
              {
                "name": "can_access_dataset"
              },
              {
                "name": "can_transfer_token"
              },
              {
                "name": "can_do_admin"
              },
              {
                "name": "can_send_request"
              },
              {
                "name": "can_admin_request"
              }
            ],
            "decisionStrategy": "UNANIMOUS"
          }
        }
      ],
      "components": {
        "org.keycloak.userprofile.UserProfileProvider": [
          {
            "providerId": "declarative-user-profile",
            "subComponents": {},
            "config": {
              "kc.user.profile.config": [
                "{\"attributes\":[{\"name\":\"username\",\"displayName\":\"${username}\",\"validations\":{\"length\":{\"min\":3,\"max\":255},\"username-prohibited-characters\":{},\"up-username-not-idn-homograph\":{}},\"permissions\":{\"view\":[\"admin\",\"user\"],\"edit\":[\"admin\",\"user\"]}},{\"name\":\"email\",\"displayName\":\"${email}\",\"validations\":{\"email\":{},\"length\":{\"max\":255}},\"required\":{\"roles\":[\"user\"]},\"permissions\":{\"view\":[\"admin\",\"user\"],\"edit\":[\"admin\",\"user\"]}},{\"name\":\"firstName\",\"displayName\":\"${firstName}\",\"validations\":{\"length\":{\"max\":255},\"person-name-prohibited-characters\":{}},\"annotations\":{},\"permissions\":{\"view\":[\"admin\",\"user\"],\"edit\":[\"admin\",\"user\"]}},{\"name\":\"lastName\",\"displayName\":\"${lastName}\",\"validations\":{\"length\":{\"max\":255},\"person-name-prohibited-characters\":{}},\"annotations\":{},\"permissions\":{\"view\":[\"admin\",\"user\"],\"edit\":[\"admin\",\"user\"]}}],\"groups\":[{\"name\":\"user-metadata\",\"displayHeader\":\"User metadata\",\"displayDescription\":\"Attributes, which refer to user metadata\"}]}"
              ]
            }
          }
        ]
      }
    }
    
  quarkus.properties: |-
    quarkus.transaction-manager.enable-recovery=true
---
# Source: federated-node/templates/keycloak-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: keycloak
  namespace: keycloak
  # Exception, so that init jobs can pick up the new port change.
  # Need to manually add the default annotations as well
  labels:
    app.kubernetes.io/managed-by: Helm
  annotations:
    meta.helm.sh/release-name: fn
    meta.helm.sh/release-namespace: fn
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-4"
    helm.sh/resource-policy: keep
spec:
  ports:
    - port: 80
      name: http
      targetPort: 8080
      protocol: TCP
  selector:
    app: keycloak
  clusterIP: None
---
# Source: federated-node/templates/keycloak-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: keycloak
  namespace: keycloak
  # Exception, so that init jobs can pick up the new port change.
  # Need to manually add the default annotations as well
  labels:
    app.kubernetes.io/managed-by: Helm
  annotations:
    meta.helm.sh/release-name: fn
    meta.helm.sh/release-namespace: fn
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-4"
    helm.sh/resource-policy: keep
spec:
  ports:
    - port: 80
      name: http
      targetPort: 8080
      protocol: TCP
  selector:
    app: keycloak
---
# Source: federated-node/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "fn-tests"
  namespace: fn
  annotations:
    helm.sh/hook: test
    helm.sh/hook-delete-policy: before-hook-creation
spec:
  containers:
    
    - name: health-check
      image: curlimages/curl
      command: ['curl' , 'http://backend.fn.svc:5000/health_check']
    
    - name: containers
      image: curlimages/curl
      command: ['curl' , 'http://backend.fn.svc:5000/containers']
    
  restartPolicy: Never
---
# Source: federated-node/templates/keycloak-realm-controller.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: keycloak-realm-init
  namespace: keycloak
  labels:
    app: keycloak-realm-init
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "-1"
spec:
  selector:
    matchLabels:
      app: keycloak-realm-init
  template:
    metadata:
      labels:
        app: keycloak-realm-init
    spec:
      serviceAccountName: k8s-keycloak-handler
      containers:
        - name: post-install-job
          imagePullPolicy: Always
          image: "ghcr.io/aridhia-open-source/keycloak_initializer:1.7.0"
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
            capabilities:
              drop: [ "ALL" ]
          env:
            - name: KC_NAMESPACE
              value: keycloak
            - name: KC_REPLICAS
              value: "2"
            - name: KEYCLOAK_URL
              value: http://keycloak.keycloak.svc.cluster.local
          envFrom:
          - configMapRef:
              name: keycloak-config
          - secretRef:
              name: kc-secrets
---
# Source: federated-node/templates/migrations/migrate-regcreds-secrets.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: regcred-migration
  namespace: fn
  annotations:
    helm.sh/hook: post-install, post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
spec:
  backoffLimit: 10
  ttlSecondsAfterFinished: 30
  template:
    metadata:
      name: "regcred-migration"
    spec:
      restartPolicy: Never
      serviceAccountName: secret-backend-handler
      containers:
        - name: migrate
          image: ghcr.io/aridhia-open-source/alpine:1.7.0
          command: ["python3", "/usr/bin/migrate-docker-secret"]
          imagePullPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
            capabilities:
              drop: [ "ALL" ]
          env:
          - name: BACKEND_URL
            value: http://backend:5000
          - name: KEYCLOAK_ADMIN
            value: admin
          - name: DEFAULT_NAMESPACE
            value: "fn"
          - name: TASK_NAMESPACE
            value: "tasks"
          - name: KEYCLOAK_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                name: kc-secrets
                key: KEYCLOAK_ADMIN_PASSWORD
MANIFEST:
---
# Source: federated-node/templates/task-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: tasks
---
# Source: federated-node/charts/dagster/charts/dagster-user-deployments/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fn-dagster-user-deployments-user-deployments
  labels: 
    helm.sh/chart: dagster-user-deployments-1.12.5
    app.kubernetes.io/name: dagster-user-deployments
    app.kubernetes.io/instance: fn
    app.kubernetes.io/version: "1.12.5"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
automountServiceAccountToken: false
---
# Source: federated-node/charts/dagster/charts/rabbitmq/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fn-rabbitmq
  labels:
    app: rabbitmq
    chart: rabbitmq-6.16.3
    release: "fn"
    heritage: "Helm"
---
# Source: federated-node/charts/dagster/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fn-dagster
  labels: 
    helm.sh/chart: dagster-1.12.5
    app.kubernetes.io/name: dagster
    app.kubernetes.io/instance: fn
    app.kubernetes.io/version: "1.12.5"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
automountServiceAccountToken: false
---
# Source: federated-node/templates/backend-roles.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: secret-backend-handler
  namespace: fn
---
# Source: federated-node/templates/keycloak-roles.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8s-keycloak-handler
  namespace: keycloak
---
# Source: federated-node/templates/result-cleaner-cronjob.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: task-cleaner-handler
  namespace: fn
---
# Source: federated-node/charts/dagster/charts/postgresql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: fn-postgresql
  labels:
    app: postgresql
    chart: postgresql-8.1.0
    release: "fn"
    heritage: "Helm"
type: Opaque
data:
  postgresql-password: "dGVzdA=="
---
# Source: federated-node/charts/dagster/charts/rabbitmq/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: fn-rabbitmq
  labels:
    app: rabbitmq
    chart: rabbitmq-6.16.3
    release: "fn"
    heritage: "Helm"
type: Opaque
data:
  
  rabbitmq-password: "dGVzdA=="
  
  
  rabbitmq-erlang-cookie: "WTVFWTZuUVpYaVZxSTdtbFRDRmh3ajBKOGg3bDBxY3E="
---
# Source: federated-node/charts/dagster/templates/secret-celery-config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: dagster-celery-config-secret
  labels:
    app: "dagster"
    chart: "dagster-1.12.5"
    release: "fn"
    heritage: "Helm"
type: Opaque
data:
  DAGSTER_CELERY_BROKER_URL: "cHlhbXFwOi8vdGVzdDp0ZXN0QGZuLXJhYmJpdG1xOjU2NzIvLw=="
  DAGSTER_CELERY_BACKEND_URL: "cnBjOi8v"
---
# Source: federated-node/charts/dagster/templates/secret-postgres.yaml
apiVersion: v1
kind: Secret
metadata:
  name: dagster-postgresql-secret
  labels:
    app: "dagster"
    chart: "dagster-1.12.5"
    release: "fn"
    heritage: "Helm"
type: Opaque
data:
  postgresql-password: "dGVzdA=="
---
# Source: federated-node/templates/copy-secrets.yaml
## This assumes for certain secrets to be present in the release namespace
# Basically duplicates them to the keycloak's namespace
apiVersion: v1
kind: Secret
metadata:
  name: local-db
  namespace: keycloak
data:
  value: bG9jYWwtZGItc2VjcmV0
type: Opaque
---
# Source: federated-node/templates/keycloak-secrets.yaml
kind: Secret
apiVersion: v1
metadata:
    name: kc-secrets
    namespace: fn
data:
    KEYCLOAK_SECRET: "QTI3eGpHOURtOW95RnByMjQ1cDdzejFI"
    KEYCLOAK_ADMIN_PASSWORD: "Ymo5NXZuTnFJYzhpazhQTFdjRWlVTXpj"
    KC_BOOTSTRAP_ADMIN_PASSWORD: "Ymo5NXZuTnFJYzhpazhQTFdjRWlVTXpj"
type: Obscure
---
# Source: federated-node/templates/keycloak-secrets.yaml
kind: Secret
apiVersion: v1
metadata:
    name: kc-secrets
    namespace: keycloak
data:
    KEYCLOAK_SECRET: "QTI3eGpHOURtOW95RnByMjQ1cDdzejFI"
    KEYCLOAK_ADMIN_PASSWORD: "Ymo5NXZuTnFJYzhpazhQTFdjRWlVTXpj"
    KC_BOOTSTRAP_ADMIN_PASSWORD: "Ymo5NXZuTnFJYzhpazhQTFdjRWlVTXpj"
type: Obscure
---
# Source: federated-node/charts/dagster/charts/dagster-user-deployments/templates/configmap-env-shared.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fn-dagster-user-deployments-user-deployments-shared-env
  labels:
    app: dagster-user-deployments
    chart: dagster-user-deployments-1.12.5
    release: fn
    heritage: Helm
data:
  
  
  
  DAGSTER_HOME: "/opt/dagster/dagster_home"
  DAGSTER_K8S_PIPELINE_RUN_NAMESPACE: "fn"
  DAGSTER_K8S_PIPELINE_RUN_ENV_CONFIGMAP: "fn-dagster-user-deployments-pipeline-env"
---
# Source: federated-node/charts/dagster/charts/dagster-user-deployments/templates/configmap-env-user.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fn-dagster-user-deployments-iris-analysis-user-env
  labels:
    app: dagster-user-deployments
    chart: dagster-user-deployments-1.12.5
    release: fn
    heritage: Helm
data:
  # If this is a map, we write it to this configmap. If it's a list, we include it
  # directly on the container (can use more k8s spec like valueFrom).
---
# Source: federated-node/charts/dagster/charts/rabbitmq/templates/configuration.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fn-rabbitmq-config
  labels:
    app: rabbitmq
    chart: rabbitmq-6.16.3
    release: "fn"
    heritage: "Helm"
data:
  enabled_plugins: |-
    [rabbitmq_management, rabbitmq_peer_discovery_k8s, rabbitmq_auth_backend_ldap].
  rabbitmq.conf: |-
    ##username and password
    default_user=test
    default_pass=CHANGEME
    ## Clustering
    cluster_formation.peer_discovery_backend  = rabbit_peer_discovery_k8s
    cluster_formation.k8s.host = kubernetes.default.svc.cluster.local
    cluster_formation.node_cleanup.interval = 10
    cluster_formation.node_cleanup.only_log_warning = true
    cluster_partition_handling = autoheal
    # queue master locator
    queue_master_locator=min-masters
    # enable guest user
    loopback_users.guest = false
    #disk_free_limit.absolute = 50MB
    #management.load_definitions = /app/load_definition.json
---
# Source: federated-node/charts/dagster/charts/rabbitmq/templates/healthchecks.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fn-rabbitmq-healthchecks
  labels:
    app: rabbitmq
    chart: rabbitmq-6.16.3
    release: "fn"
    heritage: "Helm"
data:
  rabbitmq-health-check: |-
    #!/bin/sh
    START_FLAG=/opt/bitnami/rabbitmq/var/lib/rabbitmq/.start
    if [ -f ${START_FLAG} ]; then
        rabbitmqctl node_health_check
        RESULT=$?
        if [ $RESULT -ne 0 ]; then
          rabbitmqctl status
          exit $?
        fi
        rm -f ${START_FLAG}
        exit ${RESULT}
    fi
    rabbitmq-api-check $1 $2
  rabbitmq-api-check: |-
    #!/bin/sh
    set -e
    URL=$1
    EXPECTED=$2
    ACTUAL=$(curl --silent --show-error --fail "${URL}")
    echo "${ACTUAL}"
    test "${EXPECTED}" = "${ACTUAL}"
---
# Source: federated-node/charts/dagster/templates/configmap-celery.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fn-dagster-celery-dagster
  labels:
    app: dagster
    chart: dagster-1.12.5
    release: fn
    heritage: Helm
data:
  celery.yaml: |
    execution:
      celery:
        broker:
          env: DAGSTER_CELERY_BROKER_URL
        backend:
          env: DAGSTER_CELERY_BACKEND_URL
        config_source: 
          {}
---
# Source: federated-node/charts/dagster/templates/configmap-env-celery.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fn-dagster-celery-worker-env
  labels:
    app: dagster
    chart: dagster-1.12.5
    release: fn
    heritage: Helm
data:
  DAGSTER_HOME: "/opt/dagster/dagster_home"
  DAGSTER_K8S_PIPELINE_RUN_NAMESPACE: "fn"
  DAGSTER_K8S_PIPELINE_RUN_ENV_CONFIGMAP: "fn-dagster-pipeline-env"
  DAGSTER_K8S_PIPELINE_RUN_IMAGE: "docker.io/dagster/user-code-example:1.12.5"
  DAGSTER_K8S_PIPELINE_RUN_IMAGE_PULL_POLICY: "Always"
---
# Source: federated-node/charts/dagster/templates/configmap-env-daemon.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fn-dagster-daemon-env
  labels:
    app: dagster
    chart: dagster-1.12.5
    release: fn
    heritage: Helm
data:
  
  DAGSTER_HOME: "/opt/dagster/dagster_home"
  DAGSTER_K8S_PIPELINE_RUN_NAMESPACE: "fn"
  DAGSTER_K8S_PIPELINE_RUN_ENV_CONFIGMAP: "fn-dagster-pipeline-env"
  DAGSTER_K8S_PIPELINE_RUN_IMAGE: "docker.io/dagster/user-code-example:1.12.5"
  DAGSTER_K8S_PIPELINE_RUN_IMAGE_PULL_POLICY: "Always"
  # This is a list by default, but for backcompat it can be a map. If it's a list, the env is applied directly
  # to the container.
---
# Source: federated-node/charts/dagster/templates/configmap-env-pipeline-run.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fn-dagster-pipeline-env
  labels:
    app: dagster
    chart: dagster-1.12.5
    release: fn
    heritage: Helm
data:
  
  DAGSTER_HOME: "/opt/dagster/dagster_home"
  DAGSTER_K8S_PIPELINE_RUN_NAMESPACE: "fn"
  DAGSTER_K8S_PIPELINE_RUN_ENV_CONFIGMAP: "fn-dagster-pipeline-env"
  DAGSTER_K8S_PIPELINE_RUN_IMAGE: "docker.io/dagster/user-code-example:1.12.5"
  DAGSTER_K8S_PIPELINE_RUN_IMAGE_PULL_POLICY: "Always"
---
# Source: federated-node/charts/dagster/templates/configmap-env-webserver.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fn-dagster-webserver-env
  labels:
    app: dagster
    chart: dagster-1.12.5
    release: fn
    heritage: Helm
data:
  
  DAGSTER_HOME: "/opt/dagster/dagster_home"
  DAGSTER_K8S_PIPELINE_RUN_NAMESPACE: "fn"
  DAGSTER_K8S_PIPELINE_RUN_ENV_CONFIGMAP: "fn-dagster-pipeline-env"
  DAGSTER_K8S_PIPELINE_RUN_IMAGE: "docker.io/dagster/user-code-example:1.12.5"
  DAGSTER_K8S_PIPELINE_RUN_IMAGE_PULL_POLICY: "Always"
  # This is a list by default, but for backcompat it can be a map. If it's a list, the env is applied directly
  # to the container.
---
# Source: federated-node/charts/dagster/templates/configmap-instance.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fn-dagster-instance
  labels:
    app: dagster
    chart: dagster-1.12.5
    release: fn
    heritage: Helm
data:
  dagster.yaml: |
    scheduler:      
      module: dagster.core.scheduler
      class: DagsterDaemonScheduler

    schedule_storage:
      module: dagster_postgres.schedule_storage
      class: PostgresScheduleStorage
      config:        
        postgres_db:
          username: test
          password:
            env: DAGSTER_PG_PASSWORD
          hostname: fn-postgresql
          db_name: test
          port: 5432
          params:
            {}

    run_launcher:      
      module: dagster_celery_k8s
      class: CeleryK8sRunLauncher
      config:
        dagster_home: "/opt/dagster/dagster_home"
        instance_config_map: "fn-dagster-instance"
        postgres_password_secret: "dagster-postgresql-secret"
        job_namespace: fn
        broker:
          env: DAGSTER_CELERY_BROKER_URL
        backend:
          env: DAGSTER_CELERY_BACKEND_URL
        env_config_maps:
          - env: DAGSTER_K8S_PIPELINE_RUN_ENV_CONFIGMAP
        env_secrets:
          - dagster-celery-config-secret
      
        service_account_name: fn-dagster
        image_pull_policy: Always

    run_storage:
      module: dagster_postgres.run_storage
      class: PostgresRunStorage
      config:        
        postgres_db:
          username: test
          password:
            env: DAGSTER_PG_PASSWORD
          hostname: fn-postgresql
          db_name: test
          port: 5432
          params:
            {}

    event_log_storage:
      module: dagster_postgres.event_log
      class: PostgresEventLogStorage
      config:        
        postgres_db:
          username: test
          password:
            env: DAGSTER_PG_PASSWORD
          hostname: fn-postgresql
          db_name: test
          port: 5432
          params:
            {}
    run_coordinator:      
      module: dagster.core.run_coordinator
      class: QueuedRunCoordinator
      config:
        
        max_concurrent_runs: -1
        dequeue_use_threads: true
        dequeue_num_workers: 4
    compute_logs:      
      module: dagster.core.storage.noop_compute_log_manager
      class: NoOpComputeLogManager
    run_monitoring:
      enabled: true
      start_timeout_seconds:  300
      max_resume_run_attempts: 0
      poll_interval_seconds: 120
      free_slots_after_run_end_seconds: 0
    run_retries:
      enabled: true
    sensors:
      use_threads: true
      num_workers: 4
    schedules:
      use_threads: true
      num_workers: 4

    telemetry:
      enabled: true
---
# Source: federated-node/charts/dagster/templates/configmap-workspace.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fn-dagster-workspace-yaml
  labels:
    app: dagster
    chart: dagster-1.12.5
    release: fn
    heritage: Helm
data:
  workspace.yaml: |
    load_from:
      - grpc_server:
          host: iris-analysis
          port: 3030
          location_name: iris-analysis
---
# Source: federated-node/templates/backend-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: backend-configmap
  namespace: fn
data:
  FLASK_APP: "/app"
  PGHOST: "db.fn.svc"
  PGPORT: "5432"
  PGDATABASE: "fndb"
  PGUSER: "admin"
  KEYCLOAK_URL: "http://keycloak.keycloak.svc.cluster.local"
  DEFAULT_NAMESPACE: fn
  KEYCLOAK_NAMESPACE: keycloak
  TASK_NAMESPACE: tasks
  CLEANUP_AFTER_DAYS: "1"
  PUBLIC_URL: phems-federatednode.net
  RESULTS_PATH: /mnt/results
  TASK_POD_RESULTS_PATH: /mnt/data
  IMAGE_TAG: 1.7.0
  CLAIM_CAPACITY: 10Gi
  STORAGE_CLASS: fn-shared-results
  CRD_DOMAIN: tasks.federatednode.com
---
# Source: federated-node/templates/keycloak-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: keycloak-config
  namespace: keycloak
data:
  KC_HEALTH_ENABLED: "true"
  KC_HOSTNAME_STRICT: "false"
  KC_FEATURES: admin-fine-grained-authz:v1,token-exchange,scripts
  KC_BOOTSTRAP_ADMIN_USERNAME: tmpadmin
  KEYCLOAK_ADMIN: admin
  KC_DB_URL_HOST: db.fn.svc
  KC_DB: postgres
  KC_DB_URL_PORT: "5432"
  KC_DB_URL_DATABASE: "fn_fndb"
  KC_DB_USERNAME: "admin"
  KEYCLOAK_TOKEN_LIFE: "2592000"
  KEYCLOAK_NAMESPACE: keycloak
  KEYCLOAK_HOSTNAME: phems-federatednode.net
  KC_HOSTNAME_BACKCHANNEL_DYNAMIC: "false"
  # PROXY_ADDRESS_FORWARDING: true
---
# Source: federated-node/templates/keycloak-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: keycloak-config
  namespace: fn
data:
  KEYCLOAK_ADMIN: admin
  KC_DB_URL_HOST: db.fn.svc
  KC_DB_USERNAME: "admin"
---
# Source: federated-node/templates/db-pv.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fn-db-storage
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
volumeBindingMode: WaitForFirstConsumer
provisioner: Local
---
# Source: federated-node/templates/storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fn-shared-results
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
volumeBindingMode: WaitForFirstConsumer
provisioner: Local
---
# Source: federated-node/templates/backend-results-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: fn-backend-results-10gi-pv
spec:
  storageClassName: fn-shared-results
  accessModes:
    - ReadWriteMany
    - ReadOnlyMany
  capacity:
    storage: 10Gi
  
  local:
    path: /phems/data/flask
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/os
          operator: In
          values:
          - linux
---
# Source: federated-node/templates/db-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: fn-db-pv
spec:
  storageClassName: fn-db-storage
  accessModes:
    - ReadWriteMany
  capacity:
    storage: 1Gi
  # This should be changed to some more reliable destination
  
  local:
    path: /phems/data/db/
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/os
          operator: In
          values:
          - linux
---
# Source: federated-node/templates/backend-results-pv.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backend-results-10gi-pv-vc
  namespace: 
spec:
  storageClassName: fn-shared-results
  volumeName: fn-backend-results-10gi-pv
  resources:
    requests:
      storage: 10Gi
  accessModes:
    - ReadWriteMany
---
# Source: federated-node/templates/db-pv.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: db-volclaim
  namespace: 
spec:
  storageClassName: fn-db-storage
  # This should be changed to some more reliable destination
  volumeName: fn-db-pv
  resources:
    requests:
      storage: 100Mi
  accessModes:
    - ReadWriteMany
---
# Source: federated-node/templates/backend-roles.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fn-backend-role
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "list", "create", "patch", "delete"]
- apiGroups: ["", "batch"]
  resources: ["pods", "persistentvolumes", "persistentvolumeclaims", "jobs", "pods/exec", "pods/log"]
  verbs: ["*"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list", "create"]
---
# Source: federated-node/templates/backend-roles.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fn-backend-role-binding
subjects:
  - kind: ServiceAccount
    namespace: fn
    name: secret-backend-handler
roleRef:
  kind: ClusterRole
  name: fn-backend-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: federated-node/charts/dagster/charts/dagster-user-deployments/templates/role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fn-dagster-user-deployments-role
  labels: 
    helm.sh/chart: dagster-user-deployments-1.12.5
    app.kubernetes.io/name: dagster-user-deployments
    app.kubernetes.io/instance: fn
    app.kubernetes.io/version: "1.12.5"
    app.kubernetes.io/managed-by: Helm

# Allow the Dagster service account to read and write Kubernetes jobs, deployments, pods, and events.
rules:
  - apiGroups: ["batch"]
    resources: ["jobs"]
    verbs: ["create", "delete", "deletecollection", "get", "list", "patch", "update", "watch"]
  - apiGroups: ["batch"]
    resources: ["jobs/status"]
    verbs: ["get", "watch", "list"]
  # The empty arg "" corresponds to the core API group
  - apiGroups: [""]
    resources: ["pods", "events"]
    verbs: ["create", "delete", "deletecollection", "get", "list", "patch", "update", "watch"]
  - apiGroups: [""]
    resources: ["pods/log", "pods/status"]
    verbs: ["get", "watch", "list"]
---
# Source: federated-node/charts/dagster/charts/rabbitmq/templates/role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fn-rabbitmq-endpoint-reader
  labels:
    app: rabbitmq
    chart: rabbitmq-6.16.3
    release: "fn"
    heritage: "Helm"
rules:
- apiGroups: [""]
  resources: ["endpoints"]
  verbs: ["get"]
---
# Source: federated-node/charts/dagster/templates/role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fn-dagster-role
  labels:
    app: dagster
    chart: dagster-1.12.5
    release: "fn"
    heritage: "Helm"

# Allow the Dagster service account to read and write Kubernetes jobs, deployments, pods, and events.
rules:
  - apiGroups: ["batch"]
    resources: ["jobs"]
    verbs: ["create", "delete", "deletecollection", "get", "list", "patch", "update", "watch"]
  - apiGroups: ["batch"]
    resources: ["jobs/status"]
    verbs: ["get", "watch", "list"]
  # The empty arg "" corresponds to the core API group
  - apiGroups: [""]
    resources: ["pods", "events"]
    verbs: ["create", "delete", "deletecollection", "get", "list", "patch", "update", "watch"]
  - apiGroups: [""]
    resources: ["pods/log", "pods/status"]
    verbs: ["get", "watch", "list"]
---
# Source: federated-node/templates/keycloak-roles.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: keycloak-role
  namespace: keycloak
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
- apiGroups: ["apps"]
  resources: ["statefulsets"]
  verbs: ["get", "watch", "list"]
---
# Source: federated-node/templates/result-cleaner-cronjob.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cleaner-role
  namespace: fn
rules:
- apiGroups: ["", "batch"]
  resources: ["pods", "persistentvolumes", "persistentvolumeclaims", "jobs", "pods/exec"]
  verbs: ["*"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list", "create", "delete"]
---
# Source: federated-node/charts/dagster/charts/dagster-user-deployments/templates/rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fn-dagster-user-deployments-rolebinding
  labels: 
    helm.sh/chart: dagster-user-deployments-1.12.5
    app.kubernetes.io/name: dagster-user-deployments
    app.kubernetes.io/instance: fn
    app.kubernetes.io/version: "1.12.5"
    app.kubernetes.io/managed-by: Helm

subjects:
- kind: ServiceAccount
  name: fn-dagster-user-deployments-user-deployments
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: fn-dagster-user-deployments-role
---
# Source: federated-node/charts/dagster/charts/rabbitmq/templates/rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fn-rabbitmq-endpoint-reader
  labels:
    app: rabbitmq
    chart: rabbitmq-6.16.3
    release: "fn"
    heritage: "Helm"
subjects:
- kind: ServiceAccount
  name: fn-rabbitmq
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: fn-rabbitmq-endpoint-reader
---
# Source: federated-node/charts/dagster/templates/rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fn-dagster-rolebinding
  labels:
    app: dagster
    chart: dagster-1.12.5
    release: "fn"
    heritage: "Helm"
subjects:
- kind: ServiceAccount
  name: fn-dagster
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: fn-dagster-role
---
# Source: federated-node/templates/keycloak-roles.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: keycloak-role-binding
  namespace: keycloak
subjects:
  - kind: ServiceAccount
    namespace: keycloak
    name: k8s-keycloak-handler
roleRef:
  kind: Role
  name: keycloak-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: federated-node/templates/result-cleaner-cronjob.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cleaner-role-binding
subjects:
  - kind: ServiceAccount
    namespace: fn
    name: task-cleaner-handler
roleRef:
  kind: Role
  name: cleaner-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: federated-node/charts/dagster/charts/dagster-user-deployments/templates/service-user.yaml
apiVersion: v1
kind: Service
metadata:
  name: iris-analysis
  labels:
    helm.sh/chart: dagster-user-deployments-1.12.5
    app.kubernetes.io/name: dagster-user-deployments
    app.kubernetes.io/instance: fn
    app.kubernetes.io/version: "1.12.5"
    app.kubernetes.io/managed-by: Helm
    component: user-deployments
    deployment: iris-analysis
  annotations:
spec:
  type: "ClusterIP"
  ports:
    - port: 3030
      protocol: TCP
      name: grpc
  selector:
    app.kubernetes.io/name: dagster-user-deployments
    app.kubernetes.io/instance: fn
    component: user-deployments
    deployment: iris-analysis
---
# Source: federated-node/charts/dagster/charts/postgresql/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: fn-postgresql-headless
  labels:
    app: postgresql
    chart: postgresql-8.1.0
    release: "fn"
    heritage: "Helm"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app: postgresql
    release: "fn"
---
# Source: federated-node/charts/dagster/charts/postgresql/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: fn-postgresql
  labels:
    app: postgresql
    chart: postgresql-8.1.0
    release: "fn"
    heritage: "Helm"
spec:
  type: ClusterIP
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app: postgresql
    release: "fn"
    role: master
---
# Source: federated-node/charts/dagster/charts/rabbitmq/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: fn-rabbitmq-headless
  labels:
    app: rabbitmq
    chart: rabbitmq-6.16.3
    release: "fn"
    heritage: "Helm"
spec:
  clusterIP: None
  ports:
  - name: epmd
    port: 4369
    targetPort: epmd
  - name: amqp
    port: 5672
    targetPort: amqp
  - name: dist
    port: 25672
    targetPort: dist
  - name: stats
    port: 15672
    targetPort: stats
  selector:
    app: rabbitmq
    release: "fn"
---
# Source: federated-node/charts/dagster/charts/rabbitmq/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: fn-rabbitmq
  labels:
    app: rabbitmq
    chart: rabbitmq-6.16.3
    release: "fn"
    heritage: "Helm"
spec:
  type: ClusterIP
  ports:
  - name: epmd
    port: 4369
    targetPort: epmd
    nodePort: null
  - name: amqp
    port: 5672
    targetPort: amqp
    nodePort: null
  - name: dist
    port: 25672
    targetPort: dist
    nodePort: null
  - name: stats
    port: 15672
    targetPort: stats
    nodePort: null
  selector:
    app: rabbitmq
    release: "fn"
---
# Source: federated-node/charts/dagster/templates/service-webserver.yaml
apiVersion: v1
kind: Service
metadata:
  name: fn-dagster-webserver
  labels:
    helm.sh/chart: dagster-1.12.5
    app.kubernetes.io/name: dagster
    app.kubernetes.io/instance: fn
    app.kubernetes.io/version: "1.12.5"
    app.kubernetes.io/managed-by: Helm
    component: dagster-webserver
  annotations:
spec:
  type: ClusterIP
  ports:
    - port: 80
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: dagster
    app.kubernetes.io/instance: fn
    component: dagster-webserver
---
# Source: federated-node/templates/backend-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: backend
  namespace: fn
spec:
  type: ClusterIP
  ports:
    - port: 5000
      name: http
      targetPort: 5000
      protocol: TCP
  selector:
    app: flask
---
# Source: federated-node/templates/db-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: db
  namespace: fn
spec:
  ports:
    - port: 5432
      name: pgport
      targetPort: 5432
      protocol: TCP
  selector:
    app: postgres
---
# Source: federated-node/charts/dagster/charts/dagster-user-deployments/templates/deployment-user.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fn-dagster-user-deployments-iris-analysis
  labels:
    helm.sh/chart: dagster-user-deployments-1.12.5
    app.kubernetes.io/name: dagster-user-deployments
    app.kubernetes.io/instance: fn
    app.kubernetes.io/version: "1.12.5"
    app.kubernetes.io/managed-by: Helm
    component: user-deployments
    deployment: iris-analysis
  annotations: 
    null
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: dagster-user-deployments
      app.kubernetes.io/instance: fn
      component: user-deployments
      deployment: iris-analysis
  template:
    metadata:
      labels:
        app.kubernetes.io/name: dagster-user-deployments
        app.kubernetes.io/instance: fn
        component: user-deployments
        deployment: iris-analysis
      annotations:
        checksum/dagster-user-deployment: 0175405724c389153b47ef250864f4f048765099b797525452546ee2deab0b8b
    spec:
      imagePullSecrets: 
        []
      serviceAccountName: fn-dagster-user-deployments-user-deployments
      automountServiceAccountToken: true
      securityContext: 
        null
      containers:
        - name: dagster-user-deployments
          securityContext: 
            null
          imagePullPolicy: Always
          image: "172.24.133.247:32000/iris-analytics:v3"
          args: ["dagster", "api", "grpc", "-h", "0.0.0.0", "-p", "3030", "--python-file","/iris_analysis/definitions.py"]
          env:
            - name: DAGSTER_CURRENT_IMAGE
              value: "172.24.133.247:32000/iris-analytics:v3"
            - name: DAGSTER_PG_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: dagster-postgresql-secret
                  key: postgresql-password
            # uses the auto_envvar_prefix of the dagster cli to set the --container-context arg
            # on 'dagster api grpc'
            - name: DAGSTER_CLI_API_GRPC_CONTAINER_CONTEXT
              value: "{\"k8s\":{\"env_config_maps\":[\"fn-dagster-user-deployments-iris-analysis-user-env\"],\"image_pull_policy\":\"Always\",\"namespace\":\"fn\",\"run_k8s_config\":{\"pod_spec_config\":{\"automount_service_account_token\":true}},\"service_account_name\":\"fn-dagster-user-deployments-user-deployments\"}}"
            # If this is a map, we write it to a configmap. If it's a list, we include it here (can use more k8s spec like valueFrom).
          envFrom:
            - configMapRef:
                name: fn-dagster-user-deployments-user-deployments-shared-env
            - configMapRef:
                name: fn-dagster-user-deployments-iris-analysis-user-env
            - secretRef:
                name: dagster-celery-config-secret
                optional: true
          resources: 
            null
          volumeMounts:
        # Only disable readiness if explicitly set to false
          readinessProbe:
            exec:
              command: ["dagster", "api", "grpc-health-check", "-p", "3030"]
            periodSeconds: 20
            timeoutSeconds: 10
            successThreshold: 1
            failureThreshold: 15
      nodeSelector: 
        null
      affinity: 
        null
      tolerations:
        null
      volumes:
---
# Source: federated-node/charts/dagster/templates/deployment-celery-queues.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fn-celery-workers-dagster
  labels:
    helm.sh/chart: dagster-1.12.5
    app.kubernetes.io/name: dagster
    app.kubernetes.io/instance: fn
    app.kubernetes.io/version: "1.12.5"
    app.kubernetes.io/managed-by: Helm
    component: celery
  annotations:
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: dagster
      app.kubernetes.io/instance: fn
  template:
    metadata:
      labels:
        app.kubernetes.io/name: dagster
        app.kubernetes.io/instance: fn
      annotations:
        checksum/dagster-instance: 2afdf0fd78af465274b54aae8c48667d55efb248d74c22000eea33cb1727cff7
    spec:
      serviceAccountName: fn-dagster
      automountServiceAccountToken: true
      securityContext:
        {}
      initContainers:
        - name: check-db-ready
          image: "docker.io/library/postgres:14.6"
          imagePullPolicy: "IfNotPresent"
          command: ['sh', '-c', 'until pg_isready -h fn-postgresql -p 5432 -U test; do echo waiting for database; sleep 2; done;']
          securityContext:
            {}
        - name: check-rabbitmq-ready
          image: "docker.io/busybox:1.28"
          imagePullPolicy: "IfNotPresent"
          command: ["sh", "-c", "until wget http://test:test@fn-rabbitmq:15672/api/aliveness-test/%2F; do echo waiting for rabbitmq; sleep 2; done;"]
          securityContext:
            {}
      containers:
        - name: dagster
          securityContext:
            {}
          imagePullPolicy: Always
          image: "172.24.133.247:32000/iris-analytics:1.12.5"
          command: ["dagster-celery"]
          args: ["worker", "start", "-A", "dagster_celery_k8s.app", "-y", "/opt/dagster/dagster_home/celery-config.yaml", "-q", "dagster",]
          env:
            - name: DAGSTER_PG_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: "dagster-postgresql-secret"
                  key: postgresql-password
          envFrom:
            - configMapRef:
                name: fn-dagster-celery-worker-env
            - secretRef:
                name: dagster-celery-config-secret
          volumeMounts:
            - name: dagster-instance
              mountPath: "/opt/dagster/dagster_home/dagster.yaml"
              subPath: dagster.yaml
            - name: dagster-celery
              mountPath: "/opt/dagster/dagster_home/celery-config.yaml"
              subPath: celery.yaml
          resources:
            {}
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - dagster-celery status -A dagster_celery_k8s.app -y /opt/dagster/dagster_home/celery-config.yaml | grep "${HOSTNAME}:.*OK"
            initialDelaySeconds:
              15
            periodSeconds:
              10
            timeoutSeconds:
              10
            successThreshold:
              1
            failureThreshold:
              3
      nodeSelector:
        {}
      volumes:
        - name: dagster-instance
          configMap:
            name: fn-dagster-instance
        - name: dagster-celery
          configMap:
            name: fn-dagster-celery-dagster
---
# Source: federated-node/charts/dagster/templates/deployment-daemon.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fn-dagster-daemon
  labels:
    helm.sh/chart: dagster-1.12.5
    app.kubernetes.io/name: dagster
    app.kubernetes.io/instance: fn
    app.kubernetes.io/version: "1.12.5"
    app.kubernetes.io/managed-by: Helm
    component: dagster-daemon
    deployment: daemon
  annotations:
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: dagster
      app.kubernetes.io/instance: fn
      component: dagster-daemon
      deployment: daemon
  template:
    metadata:
      labels:
        app.kubernetes.io/name: dagster
        app.kubernetes.io/instance: fn
        component: dagster-daemon
        deployment: daemon
      annotations:
        checksum/dagster-workspace: c10677e455819304d599f6112857983dc1b763cdda4d582c94ead0e82e6e0478
        checksum/dagster-instance: 2afdf0fd78af465274b54aae8c48667d55efb248d74c22000eea33cb1727cff7
    spec:
      imagePullSecrets:
        []
      serviceAccountName: fn-dagster
      automountServiceAccountToken: true
      securityContext:
        {}
      initContainers:
        - name: check-db-ready
          image: "docker.io/library/postgres:14.6"
          imagePullPolicy: "IfNotPresent"
          command: ['sh', '-c', 'until pg_isready -h fn-postgresql -p 5432 -U test; do echo waiting for database; sleep 2; done;']
          securityContext:
            {}
          resources:
            {}
        - name: "init-user-deployment-iris-analysis"
          image: "docker.io/busybox:1.28"
          command: ['sh', '-c', "until nslookup iris-analysis; do echo waiting for user service; sleep 2; done"]
          securityContext:
            {}
      containers:
        - name: dagster
          securityContext:
            {}
          imagePullPolicy: Always
          image: "docker.io/dagster/dagster-celery-k8s:1.12.5"
          command: [
            "/bin/bash",
            "-c",
            "dagster-daemon run -w /dagster-workspace/workspace.yaml"
          ]
          env:
            - name: DAGSTER_PG_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: "dagster-postgresql-secret"
                  key: postgresql-password
            - name: DAGSTER_DAEMON_HEARTBEAT_TOLERANCE
              value: "1800"
            # This is a list by default, but for backcompat it can be a map. As a map it's written to the daemon-env
            # configmap.
          envFrom:
            - configMapRef:
                name: fn-dagster-daemon-env
            - secretRef:
                name: dagster-celery-config-secret
          volumeMounts:
            - name: dagster-instance
              mountPath: "/opt/dagster/dagster_home/dagster.yaml"
              subPath: dagster.yaml
            # Do not use `subPath` to allow the configmap to update if modified
            - name: dagster-workspace-yaml
              mountPath: "/dagster-workspace/"
          resources:
            {}
      nodeSelector:
        {}
      volumes:
        - name: dagster-instance
          configMap:
            name: fn-dagster-instance
        - name: dagster-workspace-yaml
          configMap:
            name: fn-dagster-workspace-yaml
      affinity:
        {}
      tolerations:
        []
---
# Source: federated-node/charts/dagster/templates/deployment-webserver.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fn-dagster-webserver
  labels:
    helm.sh/chart: dagster-1.12.5
    app.kubernetes.io/name: dagster
    app.kubernetes.io/instance: fn
    app.kubernetes.io/version: "1.12.5"
    app.kubernetes.io/managed-by: Helm
    component: dagster-webserver
  annotations:
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: dagster
      app.kubernetes.io/instance: fn
      component: dagster-webserver
  template:
    metadata:
      labels:
        app.kubernetes.io/name: dagster
        app.kubernetes.io/instance: fn
        component: dagster-webserver
      annotations:
        checksum/dagster-workspace: c10677e455819304d599f6112857983dc1b763cdda4d582c94ead0e82e6e0478
        checksum/dagster-instance: 2afdf0fd78af465274b54aae8c48667d55efb248d74c22000eea33cb1727cff7
    spec:
      serviceAccountName: fn-dagster
      automountServiceAccountToken: true
      securityContext:
        {}
      initContainers:
        - name: check-db-ready
          image: "docker.io/library/postgres:14.6"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until pg_isready -h fn-postgresql -p 5432 -U test; do echo waiting for database; sleep 2; done;']
          securityContext:
            {}
        - name: "init-user-deployment-iris-analysis"
          image: "docker.io/busybox:1.28"
          command: ['sh', '-c', "until nslookup iris-analysis; do echo waiting for user service; sleep 2; done"]
          securityContext:
            {}
      containers:
        - name: dagster
          securityContext:
            {}
          imagePullPolicy: Always
          image: "docker.io/dagster/dagster-celery-k8s:1.12.5"
          command: [
            "/bin/bash",
            "-c",
            "dagster-webserver -h 0.0.0.0 -p 80 -w /dagster-workspace/workspace.yaml"
          ]
          env:
            - name: DAGSTER_PG_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: "dagster-postgresql-secret"
                  key: postgresql-password
            # This is a list by default, but for backcompat it can be a map. As
            # a map it's written to the webserver-env configmap.
          envFrom:
            - configMapRef:
                name: fn-dagster-webserver-env
            - secretRef:
                name: dagster-celery-config-secret
          volumeMounts:
            - name: dagster-instance
              mountPath: "/opt/dagster/dagster_home/dagster.yaml"
              subPath: dagster.yaml
            # Do not use `subPath` to allow the configmap to update if modified
            - name: dagster-workspace-yaml
              mountPath: "/dagster-workspace/"
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          resources:
            {}
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /server_info
              port: 80
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 10

      volumes:
        - name: dagster-instance
          configMap:
            name: fn-dagster-instance
        - name: dagster-workspace-yaml
          configMap:
            name: fn-dagster-workspace-yaml
---
# Source: federated-node/templates/backend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
  namespace: fn
  labels:
    app: flask
spec:
  replicas: 1
  selector:
    matchLabels:
      app: flask
  template:
    metadata:
      annotations:
        rollme: "Rv7Pe"
      labels:
        app: flask
    spec:
      serviceAccountName: secret-backend-handler
      initContainers:
        - image: ghcr.io/aridhia-open-source/alpine:1.7.0
          name: dbinit
          command: [ "dbinit" ]
          imagePullPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
            capabilities:
              drop: [ "ALL" ]
          env:
          - name: PGUSER
            valueFrom:
              configMapKeyRef:
                name: keycloak-config
                key: KC_DB_USERNAME
          - name: PGHOST
            valueFrom:
              configMapKeyRef:
                name: keycloak-config
                key: KC_DB_URL_HOST
          - name: PGPASSWORD
            valueFrom:
              secretKeyRef:
                name: local-db
                key: value
          - name: NEW_DB
            valueFrom:
              configMapKeyRef:
                name: backend-configmap
                key: PGDATABASE
        - name: db-migrations
          image: ghcr.io/aridhia-open-source/federated_node_run:1.7.0
          command: ["/bin/sh"]
          workingDir: /
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
            capabilities:
              drop: [ "ALL" ]
          args:
            [
              "-c",
              'python -m alembic upgrade head'
            ]
          envFrom:
            - configMapRef:
                name: backend-configmap
          env:
          - name: PGPASSWORD
            valueFrom:
              secretKeyRef:
                name: local-db
                key: value
        - name: storage-init
          image: ghcr.io/aridhia-open-source/alpine:1.7.0
          command: ["/bin/sh"]
          args:
            - -c
            - mkdir -p /mnt/storage/results
          volumeMounts:
            - name: respv
              mountPath: /mnt/storage
      containers:
        - image: ghcr.io/aridhia-open-source/federated_node_run:1.7.0
          name: backend
          imagePullPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
            capabilities:
              drop: [ "ALL" ]
          resources:
            limits:
              memory: 200Mi
              cpu: 100m
          ports:
            - name: http
              containerPort: 5000
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /ready_check
              port: http
            initialDelaySeconds: 10
            periodSeconds: 15
            timeoutSeconds: 30
            failureThreshold: 5
          livenessProbe:
            httpGet:
              path: /health_check
              port: http
            initialDelaySeconds: 10
            periodSeconds: 30
            timeoutSeconds: 30
            failureThreshold: 5
          envFrom:
            - configMapRef:
                name: backend-configmap
            - configMapRef:
                name: keycloak-config
            - secretRef:
                name: kc-secrets
          env:
          - name: PGPASSWORD
            valueFrom:
              secretKeyRef:
                name: local-db
                key: value
          volumeMounts:
            - name: respv
              mountPath: /mnt/results
              subPath: results
      volumes:
        - name: respv
          persistentVolumeClaim:
            claimName: backend-results-10gi-pv-vc
---
# Source: federated-node/templates/db-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: db-internal
  namespace: fn
  labels:
    app: postgres
spec:
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
        - image: postgres:13-alpine
          name: db-internal
          env:
            - name: POSTGRES_DB
              value: "fndb"
            - name: POSTGRES_USER
              value: "admin"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: local-db
                  key: value
          volumeMounts:
            - name: data
              mountPath: /var/lib/postgresql/data
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: db-volclaim
---
# Source: federated-node/charts/dagster/charts/postgresql/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: fn-postgresql
  labels:
    app: postgresql
    chart: postgresql-8.1.0
    release: "fn"
    heritage: "Helm"
spec:
  serviceName: fn-postgresql-headless
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: postgresql
      release: "fn"
      role: master
  template:
    metadata:
      name: fn-postgresql
      labels:
        app: postgresql
        chart: postgresql-8.1.0
        release: "fn"
        heritage: "Helm"
        role: master
    spec:      
      securityContext:
        fsGroup: 1001
      initContainers:
        - name: init-chmod-data
          image: docker.io/bitnami/minideb:stretch
          imagePullPolicy: "Always"
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
          command:
            - /bin/sh
            - -c
            - |
              mkdir -p /bitnami/postgresql/data
              chmod 700 /bitnami/postgresql/data
              find /bitnami/postgresql -mindepth 0 -maxdepth 1 -not -name ".snapshot" -not -name "lost+found" | \
                xargs chown -R 1001:1001
              chmod -R 777 /dev/shm
          securityContext:
            runAsUser: 0
          volumeMounts:
            - name: data
              mountPath: /bitnami/postgresql
              subPath: 
            - name: dshm
              mountPath: /dev/shm
      containers:
        - name: fn-postgresql
          image: docker.io/library/postgres:14.6
          imagePullPolicy: "IfNotPresent"
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            - name: POSTGRES_USER
              value: "test"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: fn-postgresql
                  key: postgresql-password
            - name: POSTGRES_DB
              value: "test"
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "test" -d "test" -h 127.0.0.1 -p 5432
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  pg_isready -U "test" -d "test" -h 127.0.0.1 -p 5432
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
              subPath: 
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 1Gi
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: federated-node/charts/dagster/charts/rabbitmq/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: fn-rabbitmq
  labels:
    app: rabbitmq
    chart: rabbitmq-6.16.3
    release: "fn"
    heritage: "Helm"
spec:
  serviceName: fn-rabbitmq-headless
  podManagementPolicy: OrderedReady
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: rabbitmq
      release: "fn"
  template:
    metadata:
      labels:
        app: rabbitmq
        release: "fn"
        chart: rabbitmq-6.16.3
      annotations:
        checksum/secret: 204bce02cbb16c8ff1acc43f13688b216488d7aad9f325bb1fcf0992e94f2969
    spec:      
      serviceAccountName: fn-rabbitmq
      terminationGracePeriodSeconds: 10
      initContainers:
      - name: volume-permissions
        image: "docker.io/bitnami/minideb:stretch"
        imagePullPolicy: "IfNotPresent"
        command: ["/bin/chown", "-R", "1001:1001", "/opt/bitnami/rabbitmq/var/lib/rabbitmq"]
        securityContext:
          runAsUser: 0
        resources:
          {}
        volumeMounts:
        - name: data
          mountPath: "/opt/bitnami/rabbitmq/var/lib/rabbitmq"
      containers:
      - name: rabbitmq
        image: docker.io/bitnamilegacy/rabbitmq:3.8.12
        imagePullPolicy: "IfNotPresent"
        command:
         - bash
         - -ec
         - |
            mkdir -p /opt/bitnami/rabbitmq/.rabbitmq/
            mkdir -p /opt/bitnami/rabbitmq/etc/rabbitmq/
            touch /opt/bitnami/rabbitmq/var/lib/rabbitmq/.start
            #persist the erlang cookie in both places for server and cli tools
            echo $RABBITMQ_ERL_COOKIE > /opt/bitnami/rabbitmq/var/lib/rabbitmq/.erlang.cookie
            cp /opt/bitnami/rabbitmq/var/lib/rabbitmq/.erlang.cookie /opt/bitnami/rabbitmq/.rabbitmq/
            #change permission so only the user has access to the cookie file
            chmod 600 /opt/bitnami/rabbitmq/.rabbitmq/.erlang.cookie /opt/bitnami/rabbitmq/var/lib/rabbitmq/.erlang.cookie
            #copy the mounted configuration to both places
            cp  /opt/bitnami/rabbitmq/conf/* /opt/bitnami/rabbitmq/etc/rabbitmq
            # Apply resources limits
            ulimit -n "${RABBITMQ_ULIMIT_NOFILES}"
            #replace the default password that is generated
            sed -i "/CHANGEME/cdefault_pass=${RABBITMQ_PASSWORD//\\/\\\\}" /opt/bitnami/rabbitmq/etc/rabbitmq/rabbitmq.conf
            exec rabbitmq-server
        volumeMounts:
          - name: config-volume
            mountPath: /opt/bitnami/rabbitmq/conf
          - name: healthchecks
            mountPath: /usr/local/sbin/rabbitmq-api-check
            subPath: rabbitmq-api-check
          - name: healthchecks
            mountPath: /usr/local/sbin/rabbitmq-health-check
            subPath: rabbitmq-health-check
          - name: data
            mountPath: "/opt/bitnami/rabbitmq/var/lib/rabbitmq"
        ports:
        - name: epmd
          containerPort: 4369
        - name: amqp
          containerPort: 5672
        - name: dist
          containerPort: 25672
        - name: stats
          containerPort: 15672
        livenessProbe:
          exec:
            command:
              - sh
              - -c
              - rabbitmq-api-check "http://test:$RABBITMQ_PASSWORD@127.0.0.1:15672/api/healthchecks/node" '{"status":"ok"}'
          initialDelaySeconds: 120
          timeoutSeconds: 20
          periodSeconds: 30
          failureThreshold: 6
          successThreshold: 1
        readinessProbe:
          exec:
            command:
              - sh
              - -c
              - rabbitmq-health-check "http://test:$RABBITMQ_PASSWORD@127.0.0.1:15672/api/healthchecks/node" '{"status":"ok"}'
          initialDelaySeconds: 10
          timeoutSeconds: 20
          periodSeconds: 30
          failureThreshold: 3
          successThreshold: 1
        env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: K8S_SERVICE_NAME
            value: "fn-rabbitmq-headless"
          - name: K8S_ADDRESS_TYPE
            value: hostname
          - name: RABBITMQ_NODENAME
            value: "rabbit@$(MY_POD_NAME).$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local"
          - name: K8S_HOSTNAME_SUFFIX
            value: ".$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local"
          - name: RABBITMQ_LOGS
            value: "-"
          - name: RABBITMQ_ULIMIT_NOFILES
            value: "65536"
          - name: RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS
            value: +S 2:1
          - name: RABBITMQ_USE_LONGNAME
            value: "true"
          - name: RABBITMQ_ERL_COOKIE
            valueFrom:
              secretKeyRef:
                name: fn-rabbitmq
                key: rabbitmq-erlang-cookie
          - name: RABBITMQ_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fn-rabbitmq
                key: rabbitmq-password
      securityContext:
        fsGroup: 1001
        runAsUser: 1001
      volumes:
        - name: config-volume
          configMap:
            name: fn-rabbitmq-config
            items:
            - key: rabbitmq.conf
              path: rabbitmq.conf
            - key: enabled_plugins
              path: enabled_plugins
        - name: healthchecks
          configMap:
            name: fn-rabbitmq-healthchecks
            items:
            - key: rabbitmq-health-check
              path: rabbitmq-health-check
              mode: 111
            - key: rabbitmq-api-check
              path: rabbitmq-api-check
              mode: 111
  volumeClaimTemplates:
    - metadata:
        name: data
        labels:
          app: rabbitmq
          release: "fn"
          heritage: "Helm"
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
            requests:
              storage: "8Gi"
---
# Source: federated-node/templates/keycloak-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: keycloak
  namespace: keycloak
  labels:
    app: keycloak
spec:
  serviceName: keycloak-headless
  replicas: 2
  selector:
    matchLabels:
      app: keycloak
  template:
    metadata:
      annotations:
        rollme: "uypSL"
      labels:
        app: keycloak
    spec:
      initContainers:
        - image: ghcr.io/aridhia-open-source/alpine:1.7.0
          name: dbinit
          command: [ "dbinit" ]
          imagePullPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
            capabilities:
              drop: [ "ALL" ]
          env:
          - name: PGUSER
            valueFrom:
              configMapKeyRef:
                name: keycloak-config
                key: KC_DB_USERNAME
          - name: PGHOST
            valueFrom:
              configMapKeyRef:
                name: keycloak-config
                key: KC_DB_URL_HOST
          - name: PGPASSWORD
            valueFrom:
              secretKeyRef:
                name: local-db
                key: value
          - name: NEW_DB
            valueFrom:
              configMapKeyRef:
                name: keycloak-config
                key: KC_DB_URL_DATABASE
        - name: credentials-reset
          imagePullPolicy: Always
          image: ghcr.io/aridhia-open-source/alpine:1.7.0
          command: ["keycloak-reset"]
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
            capabilities:
              drop: [ "ALL" ]
          env:
          - name: PGPASSWORD
            valueFrom:
              secretKeyRef:
                name: local-db
                key: value
          - name: KC_NAMESPACE
            value: keycloak
          - name: PGDATABASE
            value: "fn_fndb"
          envFrom:
          - secretRef:
              name: kc-secrets
          - configMapRef:
              name: keycloak-config
      containers:
        - image: ghcr.io/aridhia-open-source/federated_keycloak:1.7.0
          args: [
            "start",
            "--import-realm",
            "--http-enabled=true",
            "--optimized"
          ]
          name: keycloak
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /health
              port: 9000
            initialDelaySeconds: 20
            periodSeconds: 10
          env:
            - name: KC_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: local-db
                  key: value
          envFrom:
          - secretRef:
              name: kc-secrets
          - configMapRef:
              name: keycloak-config
          volumeMounts:
            - name: config
              mountPath: /opt/keycloak/data/import/
            - name: quarkus
              mountPath: /opt/keycloak/conf/quarkus.properties
              subPath: quarkus.properties
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
            capabilities:
              drop: [ "ALL" ]
      volumes:
        - name: config
          configMap:
            name: keycloak-start-config
            items:
            - key: config.json
              path: config.json
        - name: quarkus
          configMap:
            name: keycloak-start-config
            items:
            - key: quarkus.properties
              path: quarkus.properties
        - name: db-init
          configMap:
            name: keycloak-start-config
            defaultMode: 0777
            items:
            - key: dbinit.sh
              path: dbinit.sh
---
# Source: federated-node/templates/registry-sync-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: registry-sync
  namespace: fn
spec:
  schedule: "0 12 * * *"
  jobTemplate:
    spec:
      ttlSecondsAfterFinished: 60
      template:
        spec:
          containers:
          - name: registry-sync
            image: ghcr.io/aridhia-open-source/alpine:1.7.0
            command: ["sync-registry"]
            imagePullPolicy: Always
            securityContext:
              allowPrivilegeEscalation: false
              runAsNonRoot: true
              seccompProfile:
                type: RuntimeDefault
              capabilities:
                drop: [ "ALL" ]
            env:
            - name: BASE_URL
              value: http://backend:5000
            - name: KEYCLOAK_ADMIN
              value: admin
            - name: KEYCLOAK_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: kc-secrets
                  key: KEYCLOAK_ADMIN_PASSWORD
          restartPolicy: Never
---
# Source: federated-node/templates/result-cleaner-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: result-cleaner
  namespace: fn
spec:
  schedule: "0 12 * * *"
  jobTemplate:
    spec:
      ttlSecondsAfterFinished: 60
      template:
        spec:
          serviceAccountName: task-cleaner-handler
          containers:
          - name: result-cleaner
            image: ghcr.io/aridhia-open-source/alpine:1.7.0
            command: ["cleanup"]
            imagePullPolicy: Always
            securityContext:
              allowPrivilegeEscalation: false
              runAsNonRoot: true
              seccompProfile:
                type: RuntimeDefault
              capabilities:
                drop: [ "ALL" ]
            env:
            - name: NAMESPACE
              value: tasks
            - name: RESULTS_PATH
              value: /mnt/results
            - name: CLEANUP_AFTER_DAYS
              value: "1"
            volumeMounts:
            - name: respv
              mountPath: /mnt/results
              subPath: results
          restartPolicy: Never
          volumes:
          - name: respv
            persistentVolumeClaim:
              claimName: backend-results-10gi-pv-vc
---
# Source: federated-node/templates/certificates/certificate.yaml
# Only rendered if SSL certificates are needed
---
# Source: federated-node/templates/certificates/issuer.yaml
# Only rendered if SSL certificates are needed
---
# Source: federated-node/templates/certificates/roles.yaml
# Conditional Roles specifically for AWS certificates
---
# Source: federated-node/templates/copy-secrets.yaml
## This is optional

NOTES:
Successfully installed federated-node version: 1.7.0
Subcharts:

You can run some smoketests with:
helm test fn -n fn --logs

The smoketest pod will stay until the next one is run.
It can be deleted with:
kubectl delete pod -n fn fn-smoke-tests

Also check out our "How to use it" article: https://github.com/Aridhia-Open-Source/PHEMS_federated_node/wiki/How-to-use-it
